\author{Nicky van Foreest and Ruben van Beesten}



\opt{justhints}{
\Opensolutionfile{hint}
}


\opt{all-solutions-at-end}{
\Opensolutionfile{hint}
\Opensolutionfile{ans}
}


\begin{document}
\maketitle
\tableofcontents

\section{Lecture 1}



\begin{exercise}
Consider 12 football players on a football field.
Eleven of them are players of FC Barcelone, the other one is an arbiter.
We select a random player, uniform.
This player must take a penalty.
The probability that a player of Barcelone scores is 70\%, for the arbiter it is 50\%.
Let $P\in \{A, B\}$ be r.v that corresponds to the selected player, and $S\in\{0,1\}$ be the score.
\begin{enumerate}
\item What is the PMF? In other words, determine $\P{P = B, S=1}$ and so on for all possibilities.
\item What is $\P{S=1}$? What is $\P{P=B}$?
\item Show that $S$ and $P$ are dependent.
\end{enumerate}
\begin{solution}
Here is the joint PMF:
\begin{align}
  \label{}
\P{P=A, S=1} &= \frac{1}{12}0.5 & \P{P=A, S=0} &= \frac{1}{12}0.5 \\
\P{P=B, S=1} &= \frac{11}{12}0.7 & \P{P=B, S=0} &= \frac{11}{12}0.3.
\end{align}
Now the marginal PMFs
\begin{align*}
\P{S=1}  &= \P{P=A, S=1} + \P{P=B, S=1} = 0.042 + 0.64 = 0.683 = 1-\P{S=0}\\
\P{P=B}  &= \frac{11}{12} = 1-\P{P=A}.
\end{align*}
For independence we take the definition.
In general, for all outcomes $x,y$ we must have that $\P{X=x, Y=y} = \P{X=x}\P{Y=y}$.
For our present example, let's check for  a particular outcome:
\begin{align*}
\P{P=B, S=1} &= \frac{11}{12}\cdot0.7 \neq \P{P=B}\P{S=1} = \frac{11}{12} \cdot 0.683
\end{align*}
The joint PMF is obviously not the same as the product of the marginals, which implies that $P$ and $S$ are not independent.
\end{solution}
\end{exercise}



An insurance company receives on a certain day two claims $X, Y \geq 0$.
We will find the PMF of the loss $Z=X+Y$ under different assumptions.

The joint CDF $F_{X,Y}$ and joint PMF $p_{X,Y}$ are assumed known.

\begin{exercise}
Why is it not interesting to consider the case $\{X=0, Y=0\}$?
\begin{solution}
When the claim sizes are $0$, then the insurance company does not receive a claim.
\end{solution}
\end{exercise}


\begin{exercise}
Find an expression for the PMF of $Z=X+Y$.
\begin{solution}
By the fundamental bridge,
\begin{align}
  \label{}
\P{Z=k}
&= \sum_{i,j} \1{i+j=k} p_{X,Y}(i,j) \\
&= \sum_{i,j} \1{i, j \geq 0} \1{j=k-i} p_{X,Y}(i,j) \\
&= \sum_{i=0}^{k} p_{X,Y}(i,k-i).
\end{align}
\end{solution}
\end{exercise}

Suppose $p_{X,Y}(i,j) = c \1{i=j}\1{1\leq i \leq 4}$.


\begin{exercise}
What is $c$?
\begin{solution}
$c=1/4$ because there are just four possible values for $i$ and $j$.
\end{solution}
\end{exercise}

\begin{exercise}
What is $F_{X}(i)$?
What is $F_{Y}(j)$?
\begin{solution}
Use marginalization:
\begin{align}
F_X(k) &=  F_{X,Y}(k, \infty ) = \sum_{i\leq k} \sum_j p_{X,Y}(i,j) \\
 &= \frac{1}{4}\sum_{i\leq k} \sum_j \1{i=j}\1{1\leq i \leq 4}\\
 &= \frac{1}{4}\sum_{i\leq k} \1{1\leq i \leq 4} \\
&=k/4,\\
F_Y(j) &= j/4.
\end{align}
\end{solution}
\end{exercise}


\begin{exercise}
Are $X$ and $Y$ dependent?  If so, why, because $1=F_{X,Y}(4,4)= F_X(4)F_Y(4)$?
\begin{solution}
  The equality in the question must hold for all $i,j$, not only for $i=j=4$.
  If you take $i=j=1$, you'll see immediately that $F_{X,Y}(1,1)\neq F_X(1)F_Y(1)$:
  \begin{align}
    \label{eq:23}
    \frac{1}{4} = F_{X,Y}(1,1) \neq F_{X}(1) F_Y(1) = \frac{1}{4}\frac{1}{4}.
  \end{align}
\end{solution}
\end{exercise}

\begin{exercise}
What is $\P{Z=k}$?
\begin{solution}
$\P{Z=2} = \P{X=1, Y=1} = 1/4 = \P{Z=4}$, etc.
$\P{Z=k} = 0$ for $k\not \in \{2, 4, 6, 8\}$.
\end{solution}
\end{exercise}


\begin{exercise}
What is $\V Z$?
\begin{solution}
Here is one approach
\begin{align}
\label{eq:3}
\V Z &= \E{Z^2} - (\E Z)^{2}\\
\E{Z^2} &= \E{(X+Y)^{2}} = \E{X^{2}} + 2\E{XY} + \E{Y^{2}} \\
(E{Z})^{2} &= (\E X + \E Y)^{2} \\
 &= (\E X)^2 + 2\E X \E Y + (\E Y)^{2} \\
&\implies \\
\V Z &= \E{Z^2} - (\E Z)^{2}\\
 &= \V X + \V Y + 2 (\E{XY} - (\E X \E Y))\\
\E{XY} &= \sum_{ij} ijp_{X,Y}(i,j) = \frac 1 4 (1 + 4 + 9 + 16) = \ldots \\
\E{X^{2}} &= \ldots
\end{align}
The numbers are for you to compute.
\end{solution}
\end{exercise}

Now take $X, Y$ iid $\sim\Unif{\{1,2,3,4\}}$ (so now no longer $p_{X,Y}(i,j) = \1{i=j}\1{1\leq i \leq 4}$).

\begin{exercise}
What is $\P{Z=4}$?
\begin{solution}
\begin{align}
\label{eq:2}
\P{Z=4}
&= \sum_{i, j} \1{i+j=4} p_{X,Y}(i,j) \\
&= \sum_{i=1}^4 \sum_{j=1}^{4} \1{j=4-i} \frac{1}{16} \\
&= \sum_{i=1}^3  \frac{1}{16} \\
&= \frac{3}{16}.
\end{align}
\end{solution}
\end{exercise}

\begin{remark}
We can  make lots of variations on this theme.
\begin{enumerate}
\item Let $X\in \{1,2,3\}$ and $Y\in \{1,2,3,4\}$.
\item Take $X\sim\Pois{\lambda}$ and $Y\sim\Pois{\mu}$. (Use the chicken-egg story)
\item We can make $X$ and $Y$ such that they are (both) continuous, i.e., have densities.
  The conceptual ideas\footnote{Unless you start digging deeper.
    Then things change drastically, but we skip this technical stuff.}
  don't change much, except that the summations become integrals.
\item Why do people often/sometimes (?)
  model the claim sizes as iid $\sim\Norm{\mu, \sigma^{2}}$?
  There is a slight problem with this model (can real claim sizes be negative?), but what is the way out?
\item The example is more versatile than you might think. Here is another interpretation.

A supermarket has 5 packets of rice on the shelf.
Two customers buy rice, with amounts $X$ and $Y$.
What is the probability of a lost sale, i.e., $\P{X+Y>5}$?
What is the expected amount lost, i.e., $\E{ \max\{X+Y - 5,0\}}$?

Here is yet another.
Two patients arrive in to the first aid of a hospital.
They need $X$ and $Y$ amounts of service, and there is one doctor.
When both patients arrive at 2 pm, what is the probability that the doctor has work in overtime (after 5 pm), i.e., $\P{X+Y > 5- 2}$?
\end{enumerate}
\end{remark}



\textsc{Here is some} extra material for you practice on 2D integration.

\begin{exercise}
We have a continuous r.v. $X\geq 0$ with finite expectation. Use 2D integration and indicators to prove that
\begin{align}
\E X = \int_{0}^{\infty} x f(x) \d x = \int_{0} G(x) \d x,
\end{align}
where $G(x)$ is the survival function.
\begin{hint}
  Check the proof of BH.4.4.8
\end{hint}
\begin{solution}
The trick is to realize that $x = \int_0^{\infty} \1{y\leq x} \d y$. Using this,
\begin{align}
\E X
&= \int_{0}^{\infty} x f(x) \d x \\
&= \int_{0}^{\infty} \int_{0}^{\infty} \1{y \leq x} f(x) \d y \d x \\
&= \int_{0}^{\infty} \int_{0}^{\infty} \1{y \leq x} f(x) \d x \d y \\
&= \int_{0}^{\infty} \int_{0}^{\infty} \1{x \geq y} f(x) \d x \d y \\
&= \int_{0}^{\infty} \int_{y}^{\infty} f(x) \d x \d y \\
&= \int_{0}^{\infty} G(y) \d y.
\end{align}
\end{solution}
\end{exercise}


\begin{exercise}
Explain that for a continuous r.v. $X$ with CDF $F$ and $a$ and $b$ (so it might be that $a>b$),
\begin{equation}
  \label{eq:7}
\P{a< X < b} = [F(b) - F(a)]^{+}.
\end{equation}
\begin{hint}
  Recall that $F\in [0, 1]$.
\end{hint}
\begin{solution}
\begin{align}
a<b & \implies \P{a< X < b} = F(b) - F(a) = [F(b) - F(a)]^{+} \\
a\geq b & \implies \P{a< X < b} =  0 = [F(b) - F(a)]^{+},
\end{align}
where the last equality follows from the fact that $F$ is increasing.
\end{solution}
\end{exercise}

\textsc{Indicators are great} functions, and I suspect you underestimated the importance of these functions.
They help to keep your formulas clean.
You can use them in computer code as logical conditions, or to help counting relevant events, something you need when numerically estimating multi-D integrals for machine learning for instance.
And, even though I(=NvF) often prefer to use figures over algebra to understand something, when it comes to integration (and reversing the sequence of integration in multiple integrals) I find indicators easier to use.

Moreover, you should know that in fact, \emph{expectation} is the fundamental concept in probability theory, and the \emph{probability of an event is defined} as
\begin{equation}
  \label{eq:4}
  \P{A} := \E{\1{A}}.
\end{equation}
Thus, the fundamental bridge is actually an application of LOTUS to indicator functions. REREAD BH.4.4!


\begin{exercise}
What is $\int_{-\infty}^{\infty} \1{0\leq x \leq 3} \d x$?
\begin{solution}
\begin{align*}
\int_{-\infty}^{\infty} \1{0\leq x \leq 3} \d x =\int_{0}^{3}  \d x  = 3.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
What is
\begin{equation}
\label{eq:5}
\int x \1{0\leq x \leq 4} \d x?
\end{equation}
\begin{solution}
\begin{align*}
\int x \1{0\leq x \leq 4} \d x  = \int_{0}^{4} x \d x = 16/2 = 8.
\end{align*}
\end{solution}
\end{exercise}

When we do an integral over a 2D surface we can first integrate over the $x$ and then over the $y$, or the other way around, whatever is the most convenient.
(There are conditions about how to handle multi-D integral, but for this course these are irrelevant.)

\begin{exercise}
What is
\begin{equation}
\label{eq:5}
\iint xy \1{0\leq x \leq 3}\1{0\leq y \leq 4} \d x \d y?
\end{equation}
\begin{solution}
\begin{align*}
\iint xy \1{0\leq x \leq 3}\1{0\leq y \leq 4} \d x \d y
&=\int_{0}^{3} x \int_{0}^{4} y \d y \d x\\
&=\int_{0}^{3} x \frac{y^{2}} 2 \biggr|_{0}^{4} \d x\\
&= \int_{0}^{3} x\cdot 8 \d x = 8\cdot 9/2 = 4\cdot 9.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
What is
\begin{align}
\label{eq:5}
\iint \1{0\leq x \leq 3} \1{0\leq y \leq 4}\1{x\leq y}\d x \d y?
\end{align}
\begin{solution}
Two solutions. First we integrate over $y$.
\begin{align}
\label{eq:5}
\iint \1{0\leq x \leq 3} \1{0\leq y \leq 4}\1{x\leq y}\d x \d y
&=\int \1{0\leq x \leq 3} \int \1{0\leq y \leq 4}\1{x\leq y}\d y \d x\\
&=\int \1{0\leq x \leq 3} \int \1{\max\{x, 0\} \leq y \leq 4}\d y \d x\\
&=\int_{0}^{3} \int_{\max\{x, 0\}}^{4}\d y \d x\\
&=\int_{0}^{3} y\biggr|_{\max\{x, 0\}}^{4} \d x\\
&=\int_{0}^{3}  (4-\max\{x, 0\}) \d x\\
&=12 - \int_{0}^{3} \max\{x, 0\} \d x\\
&=12 - \int_{0}^{3} x  \d x\\
&=12 - 9/2.
\end{align}

Let's now instead first integrate over $x$.
\begin{align}
\label{eq:5}
\iint \1{0\leq x \leq 3} \1{0\leq y \leq 4}\1{x\leq y}\d x \d y
&= \int \1{0\leq y \leq 4} \int \1{0\leq x \leq 3} \1{x\leq y}\d x \d y\\
&= \int_{0}^{4} \int \1{0\leq x \leq \min\{3, y\}}\d x \d y\\
&= \int_{0}^{4} \int_{0}^{\min\{3, y\}} \d x \d y\\
&= \int_{0}^{4} \min\{3, y\}\d y\\
&= \int_{0}^{3} \min\{3, y\}\d y + \int_{3}^{4} \min\{3, y\}\d y\\
&= \int_{0}^{3} y \d y + \int_{3}^{4}  3\d y\\
&= 9/2 + 3.
\end{align}
\end{solution}
\end{exercise}


\begin{exercise}\label{ex:2}
Take $X\sim\Unif{[1,3]}, Y\sim\Unif{[2,4]}$ and independent. Compute
\begin{equation}
  \label{eq:24}
\P{Y\leq 2X}.
\end{equation}
\begin{solution}
Take $c$ the normalization constant (why is $c=1/4$), then using the previous exercise
\begin{align}
\P{Y\leq 2X}
&=\E{\1{Y\leq 2X}} \\
&=c \int_{1}^{3}\int_{2}^{4} \1{y\leq 2x} \d y \d x \\
&=c \int_{1}^{3}\int \1{2\leq y\leq \min\{4,2x\}}  \d y \d x \\
&=c \int_{1}^{3} [\min\{4, 2x\} -2]^{+} \d x
\end{align}
Now make a drawing of the function $[\min\{4, 2x\} - 2]^{+}$ on the interval $[1,3]$ to see that
\begin{equation}
\int_{1}^{3} [\min\{4, 2x\} -2]^{+} \d x = \int_{1}^{2} (2x -2) \d x + \int_{2}^{3} (4 -2) \d x.
\end{equation}
I leave the rest of the computation to you.
\end{solution}
\end{exercise}


\section{Lecture 2}

Read the problems of \verb|memoryless\_excursions.pdf|.
All the problems in that document relate to topics discussed in Sections BH.7.1 and BH.7.2, and quite a lot of topics you have seen in the previous course on probability theory.


\section{Lecture 3}

% \begin{enumerate}
% \item Why don't you setup a github repo to collaborate on making  solutions of the book?
% \item There is a \LaTeX\/ template in the sources directory on github to help you get started with the assignment.
% \end{enumerate}

% Three topics:
% \begin{enumerate}
% \item Prediction, and correlation
% \item Simulation
% \item Indicators
% \end{enumerate}
% \clearpage


\begin{exercise}
We ask a married woman on the street her height $X$.
What does this tell us about the height $Y$ of her spouse?
We suspect that taller/smaller people choose taller/smaller partners, so, given $X$, a simple estimator $\hat Y$ of $Y$ is given by
\begin{equation*}
  \hat Y = a X + b.
\end{equation*}
(What is the sign of $a$ if taller people tend to choose taller people as spouse?)
But how to determine $a$ and $b$? A common method is to find $a$ and $b$ such that the function
\begin{equation*}
  f(a,b) = \E{(Y-\hat Y)^2}
\end{equation*}
is minimized. Show that the optimal values are such that
\begin{align*}
  \hat Y = \E Y + \rho \frac{\sigma_Y}{\sigma_X} (X - \E X),
\end{align*}
where $\rho$ is the correlation between $X$ and $Y$ and where $\sigma_X$ and $\sigma_Y$ are the standard deviations of $X$ and $Y$ respectively.

\begin{solution}
We take the partial derivatives of $f$ with respect to $a$ and $b$, and solve for $a$ and $b$. In the derivation, we use that
\begin{align}
  \label{eq:6}
\rho = \frac{\cov{X,Y}}{\sqrt{\V X \V Y}} = \frac{\cov{X,Y}}{\sigma_X \sigma_Y} \implies  \rho \frac{\sigma_{Y}}{\sigma_{X}} = \frac{\cov{X,Y}}{\V X}.
\end{align}
Hence,
  \begin{align*}
f(a,b) &= \E{(Y-\hat Y)^2} \\
 &= \E{(Y-a X - b)^2} \\
 &= \E{Y^{2}} - 2a\E{YX} - 2b\E Y + a^{2}\E{X^2} + 2 ab \E X + b^{2}\\
\partial_{a} f &=-2 \E{YX} + 2a \E{X^2} + 2 b \E X = 0 \\
&\implies a \E{X^2} =  \E{YX}  -  b \E X \\
\partial_{b} f &=-2 \E{Y}  + 2 a \E X  + 2 b= 0 \\
& \implies  b = \E Y - a \E{X}\\
a \E{X^2} &=  \E{YX}  -  \E X (\E Y - a \E X) \\
&\implies  a (\E{X^{2}} - \E X \E X)  = \E{YX} - \E X \E Y  \\
&\implies a = \frac{\cov{X,Y}}{\V X} = \rho \frac{\sigma_Y}{\sigma_X}\\
b &= \E Y - \rho \frac{\sigma_Y}{\sigma_X}\E X\\
\hat Y &= a X + b \\
&= \rho \frac{\sigma_Y}{\sigma_X} X + \E Y - \rho\frac{\sigma_Y}{\sigma_X} \E X \\
&=  \E Y + \rho \frac{\sigma_Y}{\sigma_X} (X-\E X).
  \end{align*}
What a neat formula! Memorize the derivation, at least the structure. You'll come across many more optimization problems.

What if $\rho=0$?
\end{solution}
\end{exercise}

\begin{exercise}
Using scaling laws often can help to find errors. For instance,  the prediction $\hat Y$ should not change whether we measure the height in meters or centimetres.
In view of this, explain that
\begin{align*}
  \hat Y = \E Y + \rho \frac{\V Y}{\sigma_X} (X - \E X)
\end{align*}
must be wrong.
\begin{solution}
  If we measure $X$ in centimetres instead of metres, then $X$, $\E X$ and $\sigma_X$ are all multiplied by 100, and the prediction $\hat Y$ should also be expressed in centimetres But $\V Y $ scales as length squared.
  This messes up the units.
\end{solution}
\end{exercise}




\begin{exercise}
$N$ people throw their hat in a box. After shuffling, each of them takes out a hat at random. How many people do you expect to take out their own hat (i.e., the hat they put in the box); what is the variance? In BH.7.46 you have to solve this analytically. In the exercise here you have to write a simulator for compute the expectation and variance.
\begin{solution}
Let us first do one run.
\begin{pyblock}[][numbers=left,frame=lines]
import numpy as np

np.random.seed(3)

N = 4
X = np.arange(N)
np.random.shuffle(X)
print(X)
print(np.arange(N))
print((X == np.arange(N)))
print((X == np.arange(N)).sum())
\end{pyblock}
Here are the results of the print statements: $X = \py{X}$. The matches are \py{(X == np.arange(N))}; we see that $X[1] = 1$ (recall, python arrays start at index 0, not at 1, so $X[1]$ is the second element of $X$, not the first), so that the second person picks his own hat. The number of matches is therefore 1 for this simulation.

Now put the people to work, and let them pick hats for $50$ times.
\begin{pyblock}[][numbers=left,frame=lines]
import numpy as np

np.random.seed(3)

num_samples = 50
N = 5

res = np.zeros(num_samples)
for i in range(num_samples):
    X = np.arange(N)
    np.random.shuffle(X)
    res[i] = (X == np.arange(N)).sum()

print(res.mean(), res.var())
\end{pyblock}
Here is the number of matches for each round: \py{res}
The mean and variance are as follows: $\E X = \py{res.mean()}$ and $\V X = \py{res.var()}$.

For your convenience, here's the R code
\begin{minted}[]{R}
# set seed such that results can be recreated
set.seed(42)

# number simulations and people
numSamples <- 50
N <- 5

# initialize empty result vector
res <- c()

# for loop to simulate repeatedly
for (i in 1:numSamples) {

  # shuffle the N hats
  x <- sample(1:N)

  # number of people picking own hat (element by element the vectors x and
  # 1:N are compared, which yields a vector of TRUE and FALSE, TRUE = 1 and
  # FALSE = 0)
  correctPicks <- sum(x == 1:N)

  # append the result vector by the result of the current simulation
  res <- append(res, correctPicks)
}

# printing of observed mean and variance
print(mean(res))
print(var(res))
\end{minted}

\end{solution}
\end{exercise}




\section{Lecture 4}

\newcommand{\corrr}{\text{Corr}}
\newcommand{\covv}{\text{Cov}}


\begin{exercise} BH.7.65 Let $(X_1, \ldots, X_k)$ be Multinomial with parameters $n$ and $(p_1, \ldots, p_k)$. Use indicator r.v.s to show that $\covv(X_i, X_j) = -n p_i p_j$ for $i \neq j$.

\begin{solution}
See solution manual.
\end{solution}
\end{exercise}


\begin{exercise}
Suppose $(X,Y)$ are bivariate normal distributed with mean vector $\mu = (\mu_X, \mu_Y) = (0,0)$, standard deviations $\sigma_X = \sigma_Y = 1$ and correlation $\rho_{XY}$ between $X$ and $Y$. Specify the joint pdf of $X$ and $X + Y$.
\begin{solution}
Define $V := X$ and $W := X + Y$. Observe that for any $t_V, t_W$, we have
\begin{align}
    t_V V + t_W W &= t_V X + t_W (X + Y) \\
    &= (t_V + t_W) X + t_W Y.
\end{align}
Hence, any linear combination of $V$ and $W$ is a linear combination of $X$ and $Y$. Since $(X,Y)$ is bivariate normal, every linear combination of $X$ and $Y$ is normally distributed. Hence, every linear combination of $V$ and $W$ is normally distributed. Hence, by definition, $(V,W)$ is bivariate normally distributed.

We need to compute the mean vector and covariance matrix of $(V,W)$. We have
\begin{align}
    \mu_V = \E{V} = \E{X} = \mu_X = 0,
\end{align}
and
\begin{align}
    \mu_W = \E{W} = \E{X + Y} = \mu_X + \mu_Y = 0.
\end{align}
Next, we have
\begin{align}
    \V{V} = \V{X} = \sigma_X^2 = 1,
\end{align}
and
\begin{align}
    \V{W} &= \V{X + Y} = \V{X} + \V{Y} + 2\covv(X,Y)\\
    &= 1 + 1 + 2 \rho_{XY} \sigma_X \sigma_Y = 2(1 + \rho_{XY}).
\end{align}
Finally,
\begin{align}
    \covv(V,W) &= \covv(X, X + Y) = \covv(X, X) + \covv(X, Y) \\
    &= \sigma_X^2 + \rho_{XY} \sigma_X \sigma_Y = 1 + \rho_{XY},
\end{align}
and hence,
\begin{align}
    \rho_{VW} := \corrr(V,W) &= \frac{\covv(V,W)}{\sqrt{\V{V}\V{W}}} \\
    &= \frac{1 + \rho_{XY}}{\sqrt{1 \cdot 2(1 + \rho_{XY})}} \\
    &= \sqrt{\frac{1 + \rho_{XY}}{2}}.
\end{align}
We have now specified all parameters of the bivariate normal distribution. This yields the following joint pdf:
\begin{align}
    f_{V,W}(v,w) &= \frac{1}{2\pi \sigma_V \sigma_W \tau_{VW}} \exp\left(-\frac{1}{2 \tau_{VW}^2}\left(\left(\frac{v}{\sigma_V}\right)^2 + \left(\frac{w}{\sigma_W}\right)^2 - 2 \frac{\rho_{VW}}{\sigma_V \sigma_W} vw\right) \right),
\end{align}
where $\tau_{VW} := \sqrt{1 - \rho_{VW}^2} = \sqrt{1 - \frac{1 + \rho_{XY}}{2}} = \sqrt{\frac{1 - \rho_{XY}}{2}}$ and $\sigma_V = \sqrt{\V{V}} = 1$ and $\sigma_W = \sqrt{\V{W}} = \sqrt{2(1 + \rho_{XY})}$. Hence,
\begin{align}
    f_{V,W}(v,w) &= \frac{1}{2\pi \sqrt{1 - (\rho_{XY})^2}} \exp\left(-\frac{1}{1 - \rho_{XY}}\left(v^2 + \frac{w^2}{2(1 + \rho_{XY})}-vw\right)\right).
\end{align}

\end{solution}
\end{exercise}


The following exercises will show how probability theory can be used in finance. We will look at the tradeoff between risk and return in a financial portfolio.

John is an investor who has $\$ 10,000$ to invest.
There are three stocks he can choose from.
The returns on investment $(A,B,C)$ of these three stocks over the following year (in terms of percentages) follow a multinomial distribution.
The expected returns on investment are $\mu_A = 7.5 \%$, $\mu_B = 10\%$, $\mu_C = 20\%$.
The corresponding standard deviations are $\sigma_A = 7\%$, $\sigma_B = 12 \%$ and $\sigma_C = 17\%$.
Note that risk (measured in standard deviation) increases with expected return.
The correlation coefficients between the different returns are $\rho_{AB} = 0.7$, $\rho_{AC} = -0.8$, $\rho_{BC} = -0.3$.

\begin{exercise}
Suppose the investor decides to invest $\$ 2,000$ in stock A, $\$4,000$ in stock B, $\$2,000$ in stock C and to put the remaining $\$ 2,000$ in a savings account with a zero interest rate. What the expected value of his portfolio after a year?
\begin{solution}
Let $X$ denote the value of the portfolio after a year in thousands of dollars. Then,
\begin{align}
    X &:= 2(1 + A) + 4(1 + B) + 2(1 + C) + 2 \\
    &= 10 + 2A + 4B + 2C.
\end{align}
Then,
\begin{align}
    \E{X} &= \E{ 10 + 2A + 4B + 2C } \\
    &= 10 + 2\E{A} + 4\E{B} + 2\E{C} \\
    &= 10 + 2\cdot 0.075  + 4 \cdot 0.1  + 2 \cdot 0.2 \\
    &= 10 + 0.15  + 0.4  + 0.4 \\
    &= 10.95
\end{align}
\end{solution}
\end{exercise}

\begin{exercise}
What is the standard deviation of the value of the portfolio in a year?
\begin{solution}
We have
\begin{align}
    \V{X} &= \V{ 10 + 2A + 4B + 2C} \\
    &= \V{2A} + \V{4B} + \V{2C} \\
    &\quad +2\Big( \covv(2A, 4B) + \covv(2A, 2C) + \covv(4B, 2C) \Big) \\
    &= 4\V{A} + 16\V{B} + 4\V{C}\\
    &\quad +2\Big( 8\covv(A, B) + 4\covv(A, C) + 8\covv(B, C) \Big) \\
    &= 4\sigma_A^2 + 16\sigma_B^2 + 4\sigma_C^2 \\
    &\quad +2\Big( 8\rho_{AB}\sigma_A \sigma_B + 4\rho_{AC}\sigma_A \sigma_C + 8\rho_{BC}\sigma_B \sigma_C \Big) \\
    &= 4(0.07)^2 + 16(0.12)^2 + 4(0.17)^2 \\
    &\quad +2\Big( 8(0.7)(0.07) (0.12) + 4(-0.8)(0.07)(0.17) + 8(-0.3)(0.12) (0.17)\Big) \\
    &= 0.2856.
\end{align}
So
\begin{align}
    \sigma_X = \sqrt{0.2856} = 0.5344.
\end{align}
So $X$ has a standard deviation of $\$534$.
\end{solution}
\end{exercise}

\begin{exercise}
John does not like losing money. What is his probability of having made a net loss after a year?
\begin{solution}
We need to compute the probability $\P{X \leq 10}$. We have
\begin{align}
    \P{X \leq 10}
 &= \P{ X - \mu_X \leq 10 - 10.95} \\
    &= \P{ \frac{X - \mu_X}{\sigma_X} \leq \frac{10 - 10.95}{0.5344} } \\
    &= \P{ Z \leq \frac{10 - 10.95}{0.5344} } \\
    &= \P{ Z \leq -1.9625 } \\
    &= 0.0377.
\end{align}
So John has a probability of $3.77\%$ of losing money with his investment.
\end{solution}
\end{exercise}

John has a friend named Mary, who is a first-year EOR student. She has never invested money herself, but she is paying close attention during the course Probability Distributions. She tells her friend: ``John, your investment plan does not make a lot of sense. You can easily get a higher expected return at a lower level of risk!''

\begin{exercise}
Show that Mary is right. That is, make a portfolio with a higher expected return, but with a lower standard deviation. \\
\textit{Hint: Make use of the \textbf{negative correlation} between $C$ and the other two stocks!}
\begin{solution}
Observe that $C$ has the highest expected return \textit{and} it is negatively correlated with the other two stocks. We will use these facts to our advantage.

Starting out with portfolio $X$, we construct a portfolio $Y$ by splitting the investment in stock B in two halves, which we add to our investments in stock A and C. Since the average expected return of A and C is higher than that of B, we must have that $\E{Y} > \E{X}$. Moreover, the fact that A and C are negatively correlated will mitigate the level of risk. If one stock goes up, we expect the other to go down, so the stocks cancel out each others variability. This is the idea behind the investment principle of \textit{diversification}.

Mathematically, we define
\begin{align}
    Y &:= 4(1 + A) + 4(1 + C) + 2 \\
    &= 10 + 4A + 4C.
\end{align}
Then,
\begin{align}
    \E{Y} &= \E{10 + 4A + 4C} \\
    &= 10 + 4\E{A} + 4\E{C} \\
    &= 10 + 4(0.075) + 4(0.20) \\
    &= 11.1
\end{align}
Moreover,
\begin{align}
    \V{Y} &= \V{10 + 4A + 4C} \\
    &= \V{4A} + \V{4C} + 2 \covv(4A, 4C)\\
    &= 4^2\V{A} + 4^2 \V{C} +2 \cdot  4 \cdot 4 \cdot \covv(A,C) \\
    &= 16 (.07)^2 + 16 (.17)^2  + 32 (-.8)(.07)(.17) \\
    &= 0.23616,
\end{align}
which corresponds to a standard deviation of
\begin{align}
    \sigma_Y = \sqrt{\V Y} = \sqrt{0.23616} = 0.4860
\end{align}
So indeed, $\E{Y} > \E{X}$, while $\sigma_Y < \sigma_X$. Clearly, portfolio $Y$ is more desirable.
\end{solution}
\end{exercise}

\section{Lecture 5}
\label{sec:lecture-5}

% Remarks:
% \begin{enumerate}
% \item Sometimes the book speaks about `dreadful integrals', and it goes at length to avoid computing them. In my opinion this is not good.
% \item Avoiding analysis requires nearly always finding clever tricks, but, Clever tricks do not generalize; Clever tricks are error prone, and even can trip up (very) clever people; clever tricks are psychologically not helpful to people new in the field, e.g., students,
%   (`I could have never come up with this idea.')
% \item So, I prefer not to use clever tricks but propose to reduce the problem to an analysis problem, and use plain computations.
% \item I assume you also read BH and my notes. In particular the intro remarks on BH.8.
% \item I added exercises at the end of the material of lecture 1 so that you practice with 2D integration, and moved the exercises on this topic of lecture 3 also to the material of lecture 1.
% \end{enumerate}

% \clearpage

% Plan for today:
% \begin{enumerate}
% \item A derivation of the normal distribution.
% \item Why is there a $\pi$ in the normalization constant of the normal distribution?
% \item The first  step in  the analysis of Benford's law.
% \end{enumerate}

% \clearpage

\textsc{Here is a nice} geometrical explanation of how the normal distribution originates.

\begin{exercise}\label{ex:1}
Suppose $z_0=(x_0,y_{0})$ is the target on a dart board at which Barney (our national darts hero) aims, but you can also interpret it as the true position of a star in the sky.
Let $z$ be the actual position at which the dart of Barney lands on the board, or the measured position of the star.
For ease, take $z_0$ as the origin, i.e., $z_0=(0,0)$.
Then make the following assumptions:
\begin{enumerate}
\item The disturbance $(x,y)$ has the same distribution in any direction.
\item The disturbance $(x,y)$ along the $x$ direction and the $y$ direction are independent.
\item Large disturbances are less likely than small disturbances.
\end{enumerate}
Show that the disturbance along the $x$-axis (hence $y$-axis) is normally distributed. You can use BH.8.17 as a source of inspiration. (This is perhaps a hard exercise, but  the solution is easy to understand and very useful to memorize.)

\begin{solution}
Since the disturbance $(x,y)$ has the same distribution in any direction, it has in particular the same distribution in the $x$ and $y$ direction.
From this and property 2 we conclude that the joint PDF of the disturbance $(x,y)$ must satisfy
\begin{equation}
  \label{eq:1}
  f_{X,Y}(x,y) = f_X(x)f_Y(x) =: f(x)f(y),
\end{equation}
where we use property 2 first and then property 1, and we write $f(x)$ for ease.
Since the disturbance has the same distribution in \textit{any} direction, the density $f$ can only depend on the distance $r$ from the origin but not on the angle. Therefore, the probability that the dart lands on some square $\d x\d y$ must be such that
\begin{equation}
  \label{eq:4}
  f(x)f(y) \d x \d y = g(r) \d x \d y,
\end{equation}
for some function $g$, hence $g(r) = f(x)f(y)$. But since $g$ does not depend on the angle $\phi$,
\begin{equation}
\label{eq:5}
\partial_{\phi} g(r) = 0 = f(x) \partial_{\phi}f(y) + f(y) \partial_{\phi}f(x).
\end{equation}

What can we about $\partial_{\phi} f(x)$ and $\partial_{\phi}f(y)$?
The relation between $x$ and $y$ and $r$ and $\phi$ is given by the relations:
\begin{align}
\label{eq:6}
x &= r \cos \phi, & y&=r\sin \phi.
\end{align}
Using the chain rule,
\begin{align}
  \label{eq:7}
  \partial_{\phi} f(x) &= \partial_{x} f(x) \frac{\d x}{\d \phi} = f'(x) r (-\sin \phi) = - f'(x) y, \\
  \partial_{\phi} f(y) &= \partial_{y} f(y) \frac{\d y}{\d \phi} = f'(y) r \cos \phi =  f'(y) x.
\end{align}
All this gives for \cref{eq:5}
\begin{equation}
\label{eq:8}
0 = x f(x) f'(y) - y f(y)f'(x).
\end{equation}
Simplifying,
\begin{equation}
  \label{eq:9}
   \frac{f'(x)}{x f(x)} = \frac{f'(y)}{ y f(y)}.
\end{equation}
But now notice that must hold for all $x$ and $y$ at the same time. The only possibility is that there is some constant $\alpha$ such that
\begin{equation}
\label{eq:10}
   \frac{f'(x)}{x f(x)} =  \frac{f'(y)}{y f(y)} = \alpha.
\end{equation}
Hence, our $f$ must satisfy for all $x$
\begin{equation}
\label{eq:11}
f'(x) = \alpha x f(x).
\end{equation}
Differentiating the guess $f(x) = a e^{ x^2/{2 \alpha}}$, for some constant $a$, shows that this $f$ satisfies this differential equation.

Finally, by the third property, we want that $f$ decays as $x$ increases, so that necessarily $\alpha<0$.


We set $\alpha = -1/2\sigma^{2}$ to get the final answer:
\begin{equation}
  \label{eq:12}
  f(x) = a e^{-x^{2}/2 \sigma^{2}}.
\end{equation}
It remains to find the normalization constant $a$; recall, $f$ must be a PDF. This is the topic of the next exercise.
\end{solution}
\end{exercise}

We next find the normalizing constant of the normal distribution (and thereby offer an opportunity to practice with change of variables).

\begin{exercise}
For this purpose consider two circles in the plane: $C(N)$ with radius $N$ and $C(\sqrt 2 N)$ with radius $\sqrt 2 N$.
It is obvious that the square $S(N) = [-N,N]\times[-N,N]$ contains the first circle, and is contained in the second.
Therefore,
\begin{equation}
  \label{eq:13}
  \iint_{C(N)}f_{X,Y}(x,y) \d x\d y \leq
  \iint_{S(N)}f_{X,Y}(x,y) \d x\d y \leq
  \iint_{C(\sqrt 2 N)}f_{X,Y}(x,y) \d x\d y.
\end{equation}
Now substitute the normal distribution of~\cref{ex:1}.
Then use polar coordinates (See BH.8.1.9) to solve the integrals over the circles, and derive the normalization constant.
\begin{solution}
\begin{equation}
\label{eq:14}
  \iint_{C(N)}f_{X,Y}(x,y) \d x\d y =
a^{2}  \iint_{C(N)} e^{-(x^{2}+y^{2})/2\sigma} \d x\d y.
\end{equation}
Since  $x = r \cos \phi$ and $y=r\sin \phi$, we get that $x^2+y^2 = r^{2}$. For the Jacobian,
\begin{equation}
  \label{eq:15}
  \frac{\partial(x, y)}{\partial(r,\phi)} =
  \begin{vmatrix}
    \cos \phi  & -r\sin \phi \\
    \sin \phi  & r\cos \phi
  \end{vmatrix}
= r(\cos^{2} \phi + \sin^2 \phi) = r.
\end{equation}
Therefore
\begin{equation}
\label{eq:16}
\d x \d y = r \d r \d \phi,
\end{equation}
from which
\begin{align}
a^{2}  \iint_{C(N)} e^{-(x^{2}+y^{2})/2\sigma} \d x\d y
&=a^{2}  \iint_{C(N)} e^{-r^{2}/2\sigma^{2}} r \d r\d \phi \\
&= a^{2}  \int_{0}^{N} \int_{0}^{2\pi} e^{-r^{2}/2\sigma^{2}} r \d r\d \phi \\
&= a^{2}  2\pi \int_{0}^{N}  e^{-r^{2}/2\sigma^{2}} r \d r \\
&= - a^{2}  2\pi{\sigma^{2}} e^{-r^{2}/2\sigma^{2}}\biggr|_{0}^{N} \\
&= a^{2}2\pi{\sigma^{2}} (1-e^{-N^{2}/2\sigma^{2}}),
\end{align}
where we use~\cref{eq:11}.


Therefore, for the square,
\begin{equation}
a^{2}2\pi{\sigma^{2}} (1-e^{-N^{2}/2\sigma^{2}}) \leq
  \iint_{S(N)}f_{X,Y}(x,y) \d x\d y \leq
a^{2}2\pi{\sigma^{2}} (1-e^{-2N^{2}/2\sigma^{2}}).
\end{equation}
Taking $N\to\infty$ we conclude that
\begin{align}
a^{2}2\pi{\sigma^{2}}
&=\iint f_{X,Y}(x,y) \d x\d y
=a^{2}  \iint e^{-x^{2}/2\sigma^{2}} e^{-y^{2}/2\sigma} \d x\d y\\
&=a^{2}  \int_{-\infty}^{\infty} \int_{-\infty}^{\infty }e^{-x^{2}/2\sigma^{2}} e^{-y^{2}/2\sigma} \d x\d y
=a^{2} \left( \int_{-\infty}^{\infty }e^{-x^{2}/2\sigma^{2}} \d x\right)^{2},
\end{align}
and therefore
\begin{equation}
\label{eq:18}
\int_{-\infty}^{\infty }e^{-x^{2}/2\sigma^{2}} \d x = \sqrt{2 \pi}\sigma.
\end{equation}
\end{solution}
\end{exercise}


\textsc{Benford's law makes} a statement on the first significant digit of numbers.
Look it up on the web; it is a fascinating law.
It's used to detect fraud by insurance companies and the tax department, but also to see whether the US elections in 2020 have been rigged, or whether authorities manipulate the statistics of the number of deceased by Covid.
You can find the rest of the analysis in Section 5.5 of `The art of probability for scientists and engineers' by R.W.
Hamming. The next exercise is a first step in the analysis of Benford's law.

\begin{exercise}
Let $X,Y$ be iid with density $f$ and support $[1,10)$. Find an expression for the density of $Z=X Y$. What is the support (domain) of $Z$? If $X,Y\sim\Unif{[1,10)}$, what is $f_{Z}$?
\begin{solution}
Let's first find the density $f_{X,Z}$.
Let $g(x,y) = (x, z) = (x, x y)$, i.e., we take $z=x y$. It is simple to see that $y=z/x$.

We use the mnemonic
\begin{equation}
\label{eq:20}
f_{X,Z}(x,z) \d x \d z =
f_{X,Y}(x,y) \d x \d y
\end{equation}
to see that the density $f_{X,Z}$ must be given by
\begin{equation}
f_{X,Z}(x,z)  = f_{X,Y}(x,y) \frac{\partial(x,y)}{\partial(x, z)}.
\end{equation}
Now (I take this form because I find it easier to differentiate in this sequence),
\begin{equation}
\label{eq:19}
\left(\frac{\partial(x,y)}{\partial(x,z)}\right)^{-1} =
\frac{\partial(x,z)}{\partial(x,y)} =
\begin{vmatrix}
  1 & 0 \\
y & x
\end{vmatrix} = x.
\end{equation}
and therefore, using that $X$ and $Y$ are iid with density $f$,
\begin{equation}
f_{X,Z}(x,z)  = f_{X,Y}(x,y) \frac{1}{x} = f(x)f(y)/x = f(x)f(z/x)/ x.
\end{equation}
(Don't forget to take $1/x$ instead of $x$.)

It remains to tackle the domain of $f_{X,Z}$.
In particular, we have to account for the fact that $X, Y\in [1,10)$.
Surely, $Z\in [1, 100)$, but if $Z=80$, say, than necessarily $X>8$.
Hence, the domain is not $(x,z) \in [1,10]\times [1, 100]$, but more complicated.

We already have that $1\leq x < 10$. We also have the condition $z/x = y\in [1, 10)$, which we can simplify to a condition on $x$,
\begin{equation}
\label{eq:22}
1\leq z/x < 10 \iff 1 \geq x/z > 1/10 \iff z \geq x > z/10.
\end{equation}
Combining both constraints gives
\begin{equation}
\max\{1, z/10\} < x \leq \min\{10, z\}.
\end{equation}
All in all,
\begin{equation}
f_{X,Z}(x,z)  = f(x)f(z/x)/ x \1{\max\{1, z/10\}< x \leq \min\{10, z\}}.
\end{equation}
As a test, $z=110 \implies x > 110/10 > 10$, but the indicator says that $x\leq 10$, hence we get 0 for the indicator, which is what we want in this case. (You should test $Z=0$, $Z=1$, $Z=5$.)

I advice you to make a sketch of the support of $X$ and $Z$.


With marginalization
\begin{equation}
  \label{eq:21}
f_{Z}(z) =
\int_{1}^{10} f_{X,Z}(x,z) \d x.
\end{equation}
We can plug in the above expression, but that just results in a longer expression that we cannot solve unless we make a specific choice for $f$.

Finally, if $X,Y$ uniform on $[1,10)$, then $f(x)=1/9$, hence,
\begin{align}
  \label{eq:21}
f_{Z}(z)
&=\frac{1}{9^{2}}\int_{1}^{10}  \1{\max\{1, z/10\}< x \leq \min\{10, z\}} \frac{\d x}{x}\\
&=\frac{\log(\min\{10, z\}) - \log(\max\{1, z/10\})}{81} \1{1\leq z < 100}.
\end{align}
Do we need an indicator to ensure that $f_{X,Z}(x,z)\geq0$ for all $z \in [1, 100)$, or is this already satisfied by the expression above?

It's easy to make some interesting variations for the exam:
\begin{enumerate}
\item Change the domains or the distributions of $X$ and $Y$.
\item Take $Z=X/Y$, or $Z=X+Y$.
\end{enumerate}

\end{solution}
\end{exercise}

\section{Lecture 6}


\begin{exercise}
Let $X \sim \Pois{\lambda}$ and $Y \sim \Pois{\mu}$ be independent. What is the distribution of $Z = X + Y$?\\
\begin{hint}
Use the Binomial theorem:
\begin{align}
    (a + b)^n = \sum_{k=0}^n {n \choose k}a^k b^{n-k},
\end{align}
\textit{for any nonnegative integer $n$.}
\end{hint}

\begin{solution}
We use a convolution sum. First note that the domain of $X$ and $Y$ is $0,1,2,\ldots$. For any $n=0,1,2,\ldots$ we get
\begin{align}
    \P{Z = n} &= \sum_{k=0}^\infty \P{X = k} \P{Z = n \ | \ X = k} \\
    &= \sum_{k=0}^\infty \P{X = k} \P{Y = n - X \ | \ X = k} \\
    &= \sum_{k=0}^n \P{X = k} \P{Y = n - k} \\
    &= \sum_{k=0}^n \frac{e^{-\lambda}}{k!}\lambda^k  \cdot \frac{e^{-\mu}}{(n-k)!}\mu^{n-k}  \\
    &= e^{-(\lambda+\mu)} \sum_{k=0}^n \frac{1}{k!(n-k)!} \cdot \lambda^k\mu^{n-k}  \\
    &= \frac{e^{-(\lambda+\mu)}}{n!} \sum_{k=0}^n \frac{n!}{k!(n-k)!} \cdot \lambda^k\mu^{n-k}  \\
    &= \frac{e^{-(\lambda+\mu)}}{n!} \sum_{k=0}^n {n \choose k} \cdot \lambda^k\mu^{n-k}  \\
    &= \frac{e^{-(\lambda+\mu)}}{n!} (\lambda + \mu)^k.
\end{align}
We recognize this as the PMF of a Poisson distribution with parameter $\lambda+\mu$. Hence, $Z \sim \Pois{\lambda + \mu}$.
\end{solution}
\end{exercise}

\begin{exercise}
(BH.8.4.3.) Let $X_1, X_2, \ldots$ be i.i.d. $\Exp{\lambda}$ distributed. Let $T_n = \sum_{k=1}^n X_k$. Show that $T_n$ has the following pdf:
\begin{align}
    f_{T_n}(t) = \frac{\lambda^n}{(n-1)!} t^{n-1} e^{-\lambda t}, \quad t > 0.
\end{align}
That is, show that $T_n$ follows a \emph{Gamma distribution} with parameters $n$ and $\lambda$. (We will learn about the Gamma distribution in BH.8.4.)\\
\begin{hint}
Use mathematical induction.
\end{hint}

\begin{solution}
We use mathematical induction. For $n=1$ we have $T_1 = X_1$, which follows an exponential distribution with rate $\lambda$. We get
\begin{align}
    \frac{\lambda^n}{(n-1)!} t^{n-1} e^{-\lambda t} &= \frac{\lambda^1}{0!} t^{0} e^{-\lambda t}\\
    &= \lambda e^{-\lambda t},
\end{align}
for all $t>0$. Hence, the statement is true for $n=1$. Now suppose the statement is true for $n-1 \geq 1$. That is, we assume that
\begin{align}
    f_{T_{n-1}}(t) = \frac{\lambda^{n-1}}{(n-2)!} t^{n-2} e^{-\lambda t}, \quad t > 0.
\end{align}
We need to prove that it follows that the statement holds for $n$. Note that $T_n = T_{n-1} + X_n$. Moreover, the domain of both $X_n$ and $T_{n-1}$ is $(0,\infty)$. This yields the convolution integral for all $t>0$:
\begin{align}
    f_{T_n}(t) &= \int_{-\infty}^{\infty} f_{T_{n-1}}(t - x) f_{X_n}(x) dx \\
    &= \int_0^t f_{T_{n-1}}(t - x) f_{X_n}(x) dx \\
    &= \int_0^t \frac{\lambda^{n-1}}{(n-2)!} (t-x)^{n-2} e^{-\lambda (t-x)} \lambda e^{-\lambda x} dx \\
    &= \frac{\lambda^{n} e^{-\lambda t}}{(n-2)!} \int_0^t (t-x)^{n-2}  dx \\
    &= \frac{\lambda^{n} e^{-\lambda t}}{(n-2)!} \Bigg[ -\frac{(t - x)^{n-1}}{n-1} \Bigg]_{x=0}^t \\
    &= \frac{\lambda^{n} e^{-\lambda t}}{(n-2)!} \Bigg( 0 - \frac{-t^{n-1}}{n-1} \Bigg) \\
    &= \frac{\lambda^{n} }{(n-1)!} t^{n-1} e^{-\lambda t}.
\end{align}
Hence, the statement holds for $n$. By mathematical induction, the statement holds for any $n=1,2,\ldots$.
\end{solution}
\end{exercise}

\begin{exercise}
Let $X,Y$ be i.i.d. $\mathcal{N}(0,1)$ distributed and define $Z = X + Y$. Show that $Z \sim \mathcal{N}(0,2)$ using a convolution integral.

\begin{solution}
Recall the pdf of a $\mathcal{N}(\mu,\sigma^2)$ random variable $T$:
\begin{align}
    f_T(t) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left\{-\frac{(t-\mu)^2}{2 \sigma^2}\right\}, \quad t \in \R.
\end{align}
We use a convolution integral. We have for every $z \in \R$:
\begin{align}
    f_Z(z) &= \int_{-\infty}^\infty f_Y(z - x) f_X(x) dx \\
    &= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}} \exp\left\{-\frac{(z - x)^2}{2}\right\} \frac{1}{\sqrt{2\pi}} \exp\left\{-\frac{x^2}{2 }\right\} dx \\
    &= \frac{1}{2\pi} \int_{-\infty}^\infty  \exp\left\{-\frac{z^2 - 2zx + 2 x^2}{2}\right\} dx \\
    &= \frac{1}{2\pi} \int_{-\infty}^\infty  \exp\left\{-\left[\frac{1}{2}z^2 - zx +  x^2\right]\right\} dx \\
    &= \frac{1}{2\pi} \int_{-\infty}^\infty  \exp\left\{-\left[\left(x - \frac{1}{2}z\right)^2 + \frac{1}{4}z^2\right]\right\} dx \\
    &= \frac{1}{2\pi} \exp\left\{-\frac{1}{4}z^2\right\} \int_{-\infty}^\infty  \exp\left\{-\left(x - \frac{1}{2}z\right)^2\right\} dx \\
    &= \frac{1}{\sqrt{2\pi} \cdot \sqrt{2}} \exp\left\{-\frac{z^2}{2 \cdot \sqrt{2}^2}\right\} \cdot \int_{-\infty}^\infty  \frac{1}{\sqrt{2\pi} \cdot (1/\sqrt{2})} \exp\left\{-\frac{\left(x - \tfrac{1}{2}z\right)^2}{2 \cdot (1/\sqrt{2})^2}\right\} dx \\
    &= \frac{1}{\sqrt{2\pi} \cdot \sqrt{2}} \exp\left\{-\frac{z^2}{2 \cdot \sqrt{2}^2}\right\},
\end{align}
where in the last step we recognize that the integral on the right is the integral of the pdf of a $\mathcal{N}(\tfrac{1}{2}z, 1/2)$ random variable, which integrates to one (since any pdf integrates to one). We are left with the pdf of a $\mathcal{N}(0,2)$ random variable. Hence, $Z \sim \mathcal{N}(0,2)$.
\end{solution}
\end{exercise}



\section{Lecture 7}
\label{sec:lecture-7}

\begin{exercise}
At the end of Story 2 of Bayes' billiards (BH.8.3.2) there is the expression
\begin{equation}
\label{eq:2}
\beta(a,b) = \frac{(a-1)!(b-1)!}{(a+b-1)!}.
\end{equation}
Derive this equation.
\begin{solution}
We have by the Bayes' billiard stories that
\begin{align}
  \label{}
{n \choose k} \int_0^1 p^k(1-p)^{n-k}\d p = \frac{1}{n+1},
\end{align}
but the integral is equal to $\beta(k+1, n-k+1)$ by the definition of the $\beta$ function. Hence,
\begin{align*}
\beta(k+1, n-k+1)
&= \frac{1}{(n+1) {n \choose k}} = \frac{(n-k)!k!}{(n+1)!}.
\end{align*}
Taking $a-1 = k$, $b-1 =n -k$, then $n=b-1+k = a+b-2$, so that
\begin{align*}
\beta(a, b) = \frac{(b-1)!(a-1)!}{(a+b-1)!}.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
In the Beta-Binomial conjugacy story, BH take as prior $f(p) = \1{p\in [0,1]}$, and then they remark that when $f(p) \sim \beta(a,b)$ for general $a, b \in \N$, we must obtain the negative hypergeometric distribution.
I found this pretty intriguing, so my question is: Relate the Bayes' billiards story to the story of the Negative Hypergeometric distribution, and, in passing, provide an interpretation of $a$ and $b$ in terms of white and black balls.
Before trying to answer this question, look up the details of the negative hypergeometric distribution.
(In other words, this exercise is meant to help you sort out the details of the remark of BH about the negative hypergeometric distribution.)
\begin{solution}
  Let's follow the Bayes' billiards story no 1, but in the context of the negative hypergeometric distribution.
  We have $w$ white balls, and $b$ black balls.
  We place, arbitrarily all balls on a line, and mark the $r$th, $r\leq w$, white ball.
  Call this ball the pivot.
  Then we ask:  what is the probability that the number of black balls $X$ lying at the left of the pivot is equal to $k$, i.e, $\P{X=k}$?

Conditional on the position of the pivot being $p$, we must have that
\begin{equation}
\label{eq:3}
\P{X=k} = w \int_0^1{b \choose k} p^k(1-p)^{b-k} \times {w -1 \choose r-1}p^{r-1} (1-p)^{w-r} \d p.
\end{equation}
To see, reason like this.
There are $w$ ways to choose one of the white balls as the pivot.
(I initially forgot this factor.)
Then, given $p$, $k$ black balls lie to the left of the pivot; the rest lies to the right.
Then, of the $w-1$ remaining white balls, $r-1$ also have to lie the left of the pivot, and the others to the right (observe that $w-1 - (r-1) = w -r$, hence $(1-p)^{w-r}$.)

With the definition of the the $\beta$ function, we can simplify to this:
\begin{align}
\label{eq:3}
\P{X=k}
&= w{b \choose k} {w -1 \choose r-1}  \int_0^1 p^{k+r-1}(1-p)^{b-k+w-r} \d p \\
&= w{b \choose k} {w -1 \choose r-1}  \beta(k+r, b-k+w-r + 1)\\
&= w{b \choose k} {w -1 \choose r-1} \frac{(r+k-1)! (b-k+w-r)!}{(w+b)!}.
\end{align}

Now, from the story of the negative hypergeometric distribution, we have that
\begin{align}
\label{eq:17}
\P{X=k}
&= \frac{{w \choose r-1}{b \choose k}}{{w+b \choose r+k -1}}\frac{w-r+1}{w+b-r-k+1} \\
&= {w \choose r-1}{b \choose k} \frac{(r+k-1)!(w+b-r-k+1)!}{(w+b)!}\frac{w-r+1}{w+b-r-k+1} \\
&= {w \choose r-1}{b \choose k} \frac{(r+k-1)!(w+b-r-k)!}{(w+b)!}(w-r+1) \\
&= w {w -1 \choose r-1}{b \choose k} \frac{(r+k-1)!(w+b-r-k)!}{(w+b)!},
\end{align}
where we use that
\begin{equation*}
  w{w-1 \choose r-1} = w\frac{ (w-1)!}{(w-r)! (r-1)!} = \frac{ w!}{(w-r)! (r-1)!} =\frac{ w!}{(w-r+1)! (r-1)!} (w-r+1)= {w\choose r-1} (w-r+1).
\end{equation*}

The results for the Bayes' billiards match with those of the negative hypergeometric distribution!

So, inspecting~\cref{eq:3} we see that the $a$ and $b$ parameters in the $\beta$ distribution for the prior $f(p)$ are such that $a=r$ and $b=w-r+1$.
(Don't confuse the $b$ here with the number of black balls.
I took the $b$ to stick to the notation of the book.)
In other words, the $a$ corresponds to the white ball we mark as the pivot, and $b$ to the remaining number of white balls (plus 1).
\end{solution}
\end{exercise}

The next real exercise is about recursion applied to the negative hypergeometric distribution. But to get in the mood, here is short fun question on how to use recursion.

\begin{exercise}
We have a chocolate bar consisting of $n$ small squares.
The bar is in any shape you like, square, rectangular, whatever.
What is the number of times you have to break the bar such that you end up with the $n$ single pieces?
%Recursive arguments are very powerful, and I often find them very insightful (and I also like this type of reasoning).
\begin{solution}
  Write $T(n)$ for the number of times you have to break the bar when there are $n$ squares.
  Clearly, $T(1) =0$.
  In general, suppose $n=m+k$, $1 \leq k < n$, then $T(n) = T(m) + T(k) + 1$, because we need $T(k)$ to break the part with $k$ pieces, and $T(m)$ for the part with $m$ pieces, and we need 1 to break the big bar into the two parts. But then, with the \emph{boundary condition} $T(1)=0$,
  \begin{align*}
    T(2) &= T(1) + T(1) + 1 = 0, \\
T(3) &= T(1) + T(2) + 1 = 1 + 1 = 2,
  \end{align*}
and so on. So we guess that $T(n)=n-1$ for any $n$. Let's check. It certainly holds for $n=1$.  Generally,
\begin{align*}
n-1 = T(n) = T(k) + T(m) + 1 = k-1 + m-1 + 1 = k+m-1 = n-1.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
Use recursion to find the expected number $X$ of black balls drawn without replacement at random from an urn containing $w$ white balls and $b$ black balls before we draw 1 white ball.
In other words, I ask to compute $\E X$ for $X$ having a negative hypergeometric distribution with parameters $w,b, r=1$.

For ease, write $N(w,b)= \E X$ for an  urn with $w\geq r = 1$ white balls and $b$ black balls. Then explain that
\begin{align}
N(w, 0) &= N(0,b) = 0\quad \text{ for all $w$ and $b$},\\
  N(w,b) &= \frac{b}{w+b} (1+N(w, b-1)).
\end{align}
Then show that this implies that $N(w,b) = b/ (w+1)$.
\begin{solution}
  If there are no black balls left, then we cannot pick a black ball $\implies N(w, 0) = 0$.
  If we wait until we see the first white ball, but there are $w=0$ white balls left, we must have picked a while ball earlier (since we started with $w\geq r = 1$ white balls).
  But then we must also have stopped picking earlier $\implies N(w,0) = 0$.

When $w, b \geq 1$ two things can happen.
  Suppose we pick a white ball, then we stop right away.
  Suppose we pick a black ball, which happens with probability $b/(b+w)$, we obtain one black ball, and we continue with one black ball less. Hence, with LOTE,
\begin{equation}\label{eq:31}
  N(w,b) = \frac{b}{w+b} (1+N(w, b-1)).
\end{equation}
(You should realize that we use conditioning here. First we condition on picking a white ball, and then we analyze what happens under this condition. Then we condition on drawing a black ball, and we analyze what will happen under the assumption that is true.)

Now we need a real tiny bit of inspiration.
Let's \texttt{guess} that $N(w,b)=\alpha b$ for some $\alpha>0$.
Guessing is always allowed; if it works, we are done (since the recursion, together with the boundary condition, has a unique solution: just repeatedly applying it allows us to calculate all values).
In general, you can make any guess whatsoever, but mind that only the good guesses work. So, trying this form, we have that
\begin{equation*}
\alpha b = \frac{b}{w+b} (1+ \alpha (b-1)) \stackrel{\textrm{\tiny algebra}}{\implies} \alpha = \frac{b}{w+1}.
\end{equation*}
So,
\begin{equation*}
  N(w, b) = \frac{b }{w+1}.
\end{equation*}
This is consistent with $N(w,0)=0$, but there one tiny detail about $N(0, b)$.
In the present form, this is not 0, while it should according to the boundary condition.
The reason that we don't keep track of $r$ in our notation; we repair that in the next exercise.
Recall, we implicitly assumed that $r=1$ in $N(w,b)$.
When $r=0$, i.e., we have draw our while ball, we have stopped, and our function $N(w,b)$ no longer applies.

\end{solution}


\end{exercise}

\begin{exercise}
Extend the previous exercise to cope with the case $r\geq 2$.
For this, write $N_{r}(w,b)$ for an urn with $w$ white balls and $b$ black balls, and $r$ white balls to go.
Explain
\begin{align*}
  N_r(w,b) &= \frac{w}{w+b} N_{r-1}(w-1, b) +  \frac{b}{w+b} (1+N(w, b-1)).
\end{align*}
Then show that this implies that $N_{r}(w,b) = r b/ (w+1)$.
\begin{solution}
Observe that we start simply by taking  $r=1$, in the previous exercise, and then we generalize.

The recursion follows right away by noticing that we pick a white ball with probability $w/(w+b)$, but then we remove one white ball \emph{and} we have one white ball less to go.
With probability $b/(w+b)$ we pick a black ball, in which case the number of black balls drawn increases by one, but there is one black ball less in the urn.

If $r=2$, the recursion involves
\begin{equation*}
N_{r-1}(w-1, b) = N_{1}(w-1, b) = \frac{b }{w-1 + 1} = \frac{b }{w}.
\end{equation*}
Using this in the recursion,
\begin{align*}
  N_2(w,b)
&= \frac{w}{w+b} N_{1}(w-1, b) +  \frac{b}{w+b} (1+N(w, b-1)) \\
&= \frac{w}{w+b} \frac{b }{w} +  \frac{b}{w+b} (1+N(w, b-1)) \\
&=  \frac{b}{w+b} (2+N(w, b-1)).
\end{align*}
But this has the same form as~\cref{eq:31}, except that the 1 has been replaced by a 2. But, now we are dealing with the case $r=2$ instead of $r=1$. Hence, by the same token,
\begin{equation*}
  N_2(w,b) = 2 \frac{b}{w+1}
\end{equation*}
Knowing the result for $r=2$, we fill this for $r=3$, and so on, to get the general result.

What an elegant procedure;  we pull ourselves out of the swamp, just like Baron Munchhausen.
\end{solution}


\end{exercise}


%%% Local Variables:
%%% TeX-master: "lectures-demo"
%%% End:
